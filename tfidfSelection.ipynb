{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle, zlib\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from gensim import corpora, models\n",
    "from collections import Counter\n",
    "# Show graph\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from numpy import array\n",
    "import gensim.corpora as corpora\n",
    "import random\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from random import sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collection: Development\n",
      "Done for  0\n",
      "Done for  1\n",
      "Done for  2\n",
      "Done for  3\n",
      "Done for  4\n",
      "Done for  5\n",
      "Done for  6\n",
      "Done for  7\n",
      "Done for  8\n",
      "\n",
      "Collection: Environment\n",
      "Done for  0\n",
      "Done for  1\n",
      "Done for  2\n",
      "Done for  3\n",
      "Done for  4\n",
      "Done for  5\n",
      "Done for  6\n",
      "Done for  7\n",
      "Done for  8\n",
      "\n",
      "Collection: Industrialization\n",
      "Done for  0\n",
      "Done for  1\n",
      "Done for  2\n",
      "Done for  3\n",
      "Done for  4\n",
      "Done for  5\n",
      "Done for  6\n",
      "Done for  7\n",
      "Done for  8\n",
      "\n",
      "Collection: Lifestyle\n",
      "Done for  0\n",
      "Done for  1\n",
      "Done for  2\n",
      "Done for  3\n",
      "Done for  4\n",
      "Done for  5\n",
      "Done for  6\n",
      "Done for  7\n",
      "Done for  8\n"
     ]
    }
   ],
   "source": [
    "datasets = ['dataset_agriculture', 'dataset_development', 'dataset_environment', 'dataset_industrialization', 'dataset_lifestyle']\n",
    "models = ['model_agriculture', 'model_development', 'model_environment', 'model_industrialization', 'model_lifestyle']\n",
    "\n",
    "for dataset, model in zip(datasets[1:],models[1:]):\n",
    "\n",
    "    # Printing the collection name.\n",
    "    collection_name = dataset[8:]\n",
    "    print('\\nCollection:',collection_name.capitalize())\n",
    "\n",
    "    # Loading the dataset and the model from the drive.\n",
    "    file = open('Datasets/'+dataset, 'rb')\n",
    "    dataset = pickle.loads(zlib.decompress(pickle.load(file)))\n",
    "    file.close()\n",
    "    #model = Doc2Vec.load('../Models/'+model)\n",
    "\n",
    "    # Collecting the article_ids, and corresponding article_vectors for each class.\n",
    "    temp_ids = [[] for _ in range(9)]\n",
    "    temp_titles = [[] for _ in range(9)]\n",
    "    temp_vectors = [[] for _ in range(9)]\n",
    "    temp_datasets = [[] for _ in range(9)]\n",
    "    for i in dataset:\n",
    "        #if i[-5] in census:\n",
    "        #    continue\n",
    "        if i[6]=='Unemp' and i[-1]=='Slow':\n",
    "            if i[0] not in temp_ids[0]:\n",
    "                temp_ids[0].append(i[0])\n",
    "                temp_titles[0].append(i[1])\n",
    "                temp_vectors[0].append(i[2])\n",
    "        if i[6]=='Unemp' and i[-1]=='Average':\n",
    "            if i[0] not in temp_ids[1]:\n",
    "                temp_ids[1].append(i[0])\n",
    "                temp_titles[1].append(i[1])\n",
    "                temp_vectors[1].append(i[2])\n",
    "        if i[6]=='Unemp' and i[-1]=='Fast':\n",
    "            if i[0] not in temp_ids[2]:\n",
    "                temp_ids[2].append(i[0])\n",
    "                temp_titles[2].append(i[1])\n",
    "                temp_vectors[2].append(i[2])\n",
    "        if i[6]=='Agri' and i[-1]=='Slow':\n",
    "            if i[0] not in temp_ids[3]:\n",
    "                temp_ids[3].append(i[0])\n",
    "                temp_titles[3].append(i[1])\n",
    "                temp_vectors[3].append(i[2])\n",
    "        if i[6]=='Agri' and i[-1]=='Average':\n",
    "            if i[0] not in temp_ids[4]:\n",
    "                temp_ids[4].append(i[0])\n",
    "                temp_titles[4].append(i[1])\n",
    "                temp_vectors[4].append(i[2])\n",
    "        if i[6]=='Agri' and i[-1]=='Fast':\n",
    "            if i[0] not in temp_ids[5]:\n",
    "                temp_ids[5].append(i[0])\n",
    "                temp_titles[5].append(i[1])\n",
    "                temp_vectors[5].append(i[2])\n",
    "        if i[6]=='Non Agri' and i[-1]=='Slow':\n",
    "            if i[0] not in temp_ids[6]:\n",
    "                temp_ids[6].append(i[0])\n",
    "                temp_titles[6].append(i[1])\n",
    "                temp_vectors[6].append(i[2])\n",
    "        if i[6]=='Non Agri' and i[-1]=='Average':\n",
    "            if i[0] not in temp_ids[7]:\n",
    "                temp_ids[7].append(i[0])\n",
    "                temp_titles[7].append(i[1])\n",
    "                temp_vectors[7].append(i[2])\n",
    "        if i[6]=='Non Agri' and i[-1]=='Fast':\n",
    "            if i[0] not in temp_ids[8]:\n",
    "                temp_ids[8].append(i[0])\n",
    "                temp_titles[8].append(i[1])\n",
    "                temp_vectors[8].append(i[2])            \n",
    "    names = ['unemp_slow','unemp_avg','unemp_fast','agri_slow','agri_avg','agri_fast','non_agri_slow','non_agri_avg','non_agri_fast']\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    df_ids = pd.DataFrame()\n",
    "    corp = []\n",
    "    for i in range(9):\n",
    "        doc = ''\n",
    "        for art in temp_vectors[i]:\n",
    "            doc +=' '+art.lower()\n",
    "        corp.append(doc)\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corp)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    scrs = []\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    dd = dict(zip(features,range(len(features))))\n",
    "    for i in range(9):\n",
    "        scr = []\n",
    "        arr = X[i].toarray()\n",
    "        for art in temp_vectors[i]:\n",
    "            doc = tokenizer.tokenize(art)\n",
    "            sr = 0\n",
    "            for j in doc:\n",
    "                v = dd.get(j,-1)\n",
    "                if v==-1:\n",
    "                    continue\n",
    "                sr = sr + arr[0,v]\n",
    "            scr.append(sr)\n",
    "        print(\"Done for \",i)\n",
    "        scrs.append(scr)\n",
    "    for i in range(9):\n",
    "        idd = []\n",
    "        titles =[]\n",
    "        ind = list(np.argsort(scrs[i]))[-100:]\n",
    "        for e in ind:\n",
    "            idd.append(temp_ids[i][e])\n",
    "            titles.append(temp_titles[i][e])\n",
    "        df[names[i]+'_title'] = titles[:10]\n",
    "        df[names[i]+'_id']= idd[:10]\n",
    "        df_ids[names[i]+'_id'] = idd\n",
    "    df.to_excel('RS/'+collection_name+'.xlsx',index=False)\n",
    "    df_ids.to_excel('RS/'+collection_name+'_ids.xlsx',index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
