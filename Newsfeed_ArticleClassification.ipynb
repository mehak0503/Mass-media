{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Newsfeed-ArticleClassification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vSh873zaHOBQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fdf2bf-755c-4a46-bc47-f644b6ae52c8"
      },
      "source": [
        "# Importing the required libraries.\n",
        "import numpy as np\n",
        "import pickle, zlib\n",
        "from random import sample\n",
        "import scipy.cluster.hierarchy as sch\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim import corpora\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gensim\n",
        "# print(gensim.__version__)    # Collecting the article_ids, and corresponding article_vectors for each class.\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ui-gFOKuY7",
        "outputId": "0135fb9f-86b0-4236-afcc-315e3bac9d0e"
      },
      "source": [
        "# Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/',force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omcqW-ojwZtL"
      },
      "source": [
        "# Emp and growth types\n",
        "emp_type = ['Unemp', 'Agri', 'Non Agri']\n",
        "N_EMP = 3\n",
        "growth_type = ['Slow','Average','Fast']\n",
        "N_GROWTH = 3\n",
        "\n",
        "def ClassifyNew(df, model, MRV_Emp, MRVs):\n",
        "    PredEmp = []\n",
        "    PredGrowth = []\n",
        "    Outlier = []\n",
        "    for index, row in df.iterrows():\n",
        "        \n",
        "        txt = row['Keywords']   # Extract (pre processed) txt\n",
        "        actualEmp = row['Emp']  # Extract emp type\n",
        "\n",
        "        inferred = model.infer_vector(txt,alpha=0.1,epochs=100) # Inferring vector for this article\n",
        "\n",
        "        # Pred Emp type\n",
        "        cs = np.array([cosine_similarity(inferred.reshape(1,-1),temp.reshape(1,-1)) for temp in MRV_Emp]).reshape((N_EMP,1))\n",
        "        emp = np.argmax(cs)\n",
        "        PredEmp.append(emp_type[emp])\n",
        "\n",
        "        #Pred Pace of Growth \n",
        "        actEmp = emp_type.index(actualEmp) #(Calculate on given emp type not predicted)\n",
        "        cs = np.array([cosine_similarity(inferred.reshape(1,-1),temp.reshape(1,-1)) for temp in MRVs[N_GROWTH*actEmp:N_GROWTH*(actEmp+1)]]).reshape((N_GROWTH,1))\n",
        "        PredGrowth.append(growth_type[np.argmax(cs)])\n",
        "\n",
        "    # Add to dataframe\n",
        "    df['PredEmp'] = PredEmp\n",
        "    df['PredGrowth'] = PredGrowth\n",
        "    \n",
        "    return df"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "pDA15_iwHOCC",
        "outputId": "525b5c42-e18b-4a8a-e53d-6f4cdaa2cb05"
      },
      "source": [
        "# File paths for datasets and with appropraite folder name\n",
        "FOLDER = 'drive/My Drive/'\n",
        "PATHS = {'Test':FOLDER+'Split/Temporal/test_','Train':FOLDER+'Split/Temporal/train_','Model':FOLDER+'Split/Temporal/','Pred':FOLDER+'Split/Temporal/Prediction/Pred_'}\n",
        "\n",
        "# Datsets and models\n",
        "datasets = ['dataset_agriculture', 'dataset_development', 'dataset_environment', 'dataset_industrialization', 'dataset_lifestyle']\n",
        "models = ['model_agriculture', 'model_development', 'model_environment', 'model_industrialization', 'model_lifestyle']\n",
        "\n",
        "# For testing purposes can select specific datasets to run on\n",
        "MODE = 'Test'  #Only run on development if Mode == 'Test' else run on all\n",
        "SELECT = {'dataset_agriculture':1,'dataset_development':1,'dataset_environment':1,'dataset_industrialization':1,'dataset_lifestyle':0}\n",
        "\n",
        "SAVE = False # Save prediction file or not\n",
        "\n",
        "\n",
        "for dataset, model in zip(datasets,models):\n",
        "    if MODE=='Test' and dataset != 'dataset_development':\n",
        "        continue;\n",
        "    elif SELECT[dataset]==0:\n",
        "        continue;\n",
        "\n",
        "    # Printing the collection name.\n",
        "    collection_name = dataset[8:]\n",
        "    print('\\nCollection:',collection_name.capitalize())\n",
        "\n",
        "    # Loading the train,test dataset and the model from the drive.\n",
        "    file = open(PATHS['Train']+dataset, 'rb')\n",
        "    train_dataset = pickle.loads(zlib.decompress(pickle.load(file)))\n",
        "    file.close()\n",
        "\n",
        "    file = open(PATHS['Test']+dataset, 'rb')\n",
        "    test_dataset = pickle.loads(zlib.decompress(pickle.load(file)))\n",
        "    file.close()\n",
        "\n",
        "    model = Doc2Vec.load(PATHS['Model']+model)\n",
        "\n",
        "    ## -- CALCULATING GLOBAL CENTROID USING THE TRAIN DATASET  --##\n",
        "    dataset = train_dataset #Shorthand\n",
        "    # Collecting the article_ids, and corresponding article_vectors for each class.\n",
        "    temp_ids = [set() for _ in range(9)]\n",
        "    temp_vectors = [[] for _ in range(9)]\n",
        "    temp_datasets = [[] for _ in range(9)]\n",
        "    for i in dataset:\n",
        "        if i[6]=='Unemp' and i[7]=='Slow':\n",
        "            if i[0] not in temp_ids[0]:\n",
        "                temp_ids[0].add(i[0])\n",
        "                temp_vectors[0].append(model.docvecs[i[0]])\n",
        "                temp_datasets[0].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Unemp' and i[7]=='Average':\n",
        "            if i[0] not in temp_ids[1]:\n",
        "                temp_ids[1].add(i[0])\n",
        "                temp_vectors[1].append(model.docvecs[i[0]])\n",
        "                temp_datasets[1].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Unemp' and i[7]=='Fast':\n",
        "            if i[0] not in temp_ids[2]:\n",
        "                temp_ids[2].add(i[0])\n",
        "                temp_vectors[2].append(model.docvecs[i[0]])\n",
        "                temp_datasets[2].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Agri' and i[7]=='Slow':\n",
        "            if i[0] not in temp_ids[3]:\n",
        "                temp_ids[3].add(i[0])\n",
        "                temp_vectors[3].append(model.docvecs[i[0]])\n",
        "                temp_datasets[3].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Agri' and i[7]=='Average':\n",
        "            if i[0] not in temp_ids[4]:\n",
        "                temp_ids[4].add(i[0])\n",
        "                temp_vectors[4].append(model.docvecs[i[0]])\n",
        "                temp_datasets[4].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Agri' and i[7]=='Fast':\n",
        "            if i[0] not in temp_ids[5]:\n",
        "                temp_ids[5].add(i[0])\n",
        "                temp_vectors[5].append(model.docvecs[i[0]])\n",
        "                temp_datasets[5].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Non Agri' and i[7]=='Slow':\n",
        "            if i[0] not in temp_ids[6]:\n",
        "                temp_ids[6].add(i[0])\n",
        "                temp_vectors[6].append(model.docvecs[i[0]])\n",
        "                temp_datasets[6].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Non Agri' and i[7]=='Average':\n",
        "            if i[0] not in temp_ids[7]:\n",
        "                temp_ids[7].add(i[0])\n",
        "                temp_vectors[7].append(model.docvecs[i[0]])\n",
        "                temp_datasets[7].append([i[0],model.docvecs[i[0]]])\n",
        "        if i[6]=='Non Agri' and i[7]=='Fast':\n",
        "            if i[0] not in temp_ids[8]:\n",
        "                temp_ids[8].add(i[0])\n",
        "                temp_vectors[8].append(model.docvecs[i[0]])\n",
        "                temp_datasets[8].append([i[0],model.docvecs[i[0]]])\n",
        "\n",
        "    # Calculate Global Centroid for Emp classes as well as Growth sub classes\n",
        "    MRV_Emp = [[] for _ in range(3)]\n",
        "    MRVs=temp_vectors\n",
        "    for i in range(9):\n",
        "        MRVs[i] = np.median(MRVs[i],axis=0)\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            MRV_Emp[i].append(temp_vectors[3*i+j])\n",
        "        MRV_Emp[i] = np.median(MRV_Emp[i],axis=0)\n",
        "\n",
        "    #---X---X--- GLOBAL CENTROID CALCULATED ---X---X---#\n",
        "\n",
        "    # Set pandas options for printing\n",
        "    pd.set_option('max_colwidth', 15)\n",
        "    pd.set_option('max_rows', 10)\n",
        "\n",
        "    # Create pandas dataframe of the datasets\n",
        "    # Columns in Dataset --> 0:ArticleId, 1: Title, 2: Text, 3: Keywords(Processed text) 4: Date(YYYY,MM,DD), 5:ID, 6: Emp, 7: POG, 8: (Not used)\n",
        "    \n",
        "    # # Train (Not used)\n",
        "    # df = pd.DataFrame(dataset)\n",
        "    # df.columns = ['ArticleId','Title','Text','Keywords','Date','DistrictId','Emp','Growth','Type']\n",
        "    # df=df.drop(['Type'],axis=1)\n",
        "    # train_df = df;\n",
        "\n",
        "    # Test\n",
        "    test_df = pd.DataFrame(test_dataset)\n",
        "    test_df.columns = ['ArticleId','Title','Text','Keywords','Date','DistrictId','Emp','Growth','Type']\n",
        "    test_df=test_df.drop(['Type'],axis=1) # Not required\n",
        "\n",
        "\n",
        "    # Predicting the emp types and pace of growth\n",
        "    Pred_df = ClassifyNew(test_df, model, MRV_Emp, MRVs)\n",
        "\n",
        "    # Marking Outliers\n",
        "    Pred_df['EmpOut'] = Pred_df['Emp']!=Pred_df['PredEmp']              # Emp Outlier  \n",
        "    Pred_df['GrowthOut'] = Pred_df['Growth']!=Pred_df['PredGrowth']     # Growth Outlier\n",
        "\n",
        "    # Save Prediction File\n",
        "    if SAVE:\n",
        "        file_pred = open(PATHS['Pred']+collection_name,'wb')\n",
        "        pickle.dump(zlib.compress(pickle.dumps(Pred_df.values.tolist()),pickle.HIGHEST_PROTOCOL),file_pred,pickle.HIGHEST_PROTOCOL)\n",
        "        file_pred.close()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Collection: Development\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8a4e68590b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEMPORAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasetT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAST_VERSION\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# Collecting the article_ids, and corresponding article_vectors for each class.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mtemp_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mtemp_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'doc2vec' is not defined"
          ]
        }
      ]
    }
  ]
}
